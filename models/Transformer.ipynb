{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5a3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Set ROOT path to access other directories in project\n",
    "ROOT = Path.cwd().parent\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import SnowDepth.data_loader as DL\n",
    "import SnowDepth.data_splitter as DS\n",
    "import SnowDepth.architecture as ARCH\n",
    "import SnowDepth.optimal_features as OF\n",
    "from SnowDepth.config import HOLDOUT_AOI\n",
    "from SnowDepth.config import FEATURE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcde852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All H5 files already exist. Skipping feature selection and H5 generation.\n"
     ]
    }
   ],
   "source": [
    "# Select holdout AOI\n",
    "holdout_aoi = HOLDOUT_AOI\n",
    "\n",
    "# H5 directory and expected output files\n",
    "h5_dir = ROOT  / \"data\" / \"h5_dir\"\n",
    "h5_path_HSIC = h5_dir / \"HSIC\" / \"data_HSIC.h5\"\n",
    "h5_path_PCC  = h5_dir / \"PCC\"  / \"data_PCC.h5\"\n",
    "h5_path_MI   = h5_dir / \"MI\"   / \"data_MI.h5\"\n",
    "\n",
    "# If we have already written H5 files, we can skip a lot of steps here:\n",
    "if all(p.exists() for p in [h5_path_HSIC, h5_path_PCC, h5_path_MI]):\n",
    "    print(\"All H5 files already exist. Skipping feature selection and H5 generation.\")\n",
    "else:\n",
    "    print(\"H5 files missing. Running feature selection and writing new H5 files\")\n",
    "\n",
    "    # SET IMPORTANT VARIABLES HERE:\n",
    "    \n",
    "    # Path to TIFF files\n",
    "    data_dir = ROOT / \"data\" / \"tif_files\"\n",
    "\n",
    "    # Number of features to select\n",
    "    top_k = 10\n",
    "\n",
    "    # Load dataframe\n",
    "    df = DL.build_df(str(data_dir), drop_invalid=True, upper_threshold=3)\n",
    "    dev_df = df[df[\"aoi_name\"] != holdout_aoi].copy()\n",
    "\n",
    "    # Run feature selection\n",
    "    ff_algos = OF.optimal_feature_sets(dev_df, top_k=top_k, n_per_aoi=10000)\n",
    "\n",
    "    # Write one H5 file per feature-set\n",
    "    for name, feats in ff_algos.items():\n",
    "        out_dir = h5_dir / name\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        DL.build_h5(\n",
    "            data_dir=str(data_dir),\n",
    "            out_dir=str(out_dir),\n",
    "            write_mask=True,\n",
    "            upper_threshold=3.0,\n",
    "            selected_features=feats,\n",
    "            out_name=f\"data_{name}.h5\",\n",
    "        )\n",
    "        print(f\"Wrote new H5: {out_dir / f'data_{name}.h5'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6fc748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSIC Shapes: X_train (580, 128, 128, 10), y_train (580, 128, 128, 1), X_val (64, 128, 128, 10), y_val (64, 128, 128, 1)\n",
      "WARNING:tensorflow:From c:\\Users\\mathi\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - MAE: 0.5457 - RMSE: 0.7244 - loss: 0.2310\n",
      "Epoch 1: val_loss improved from inf to 0.07872, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 183ms/step - MAE: 0.5448 - RMSE: 0.7232 - loss: 0.2303 - val_MAE: 0.3073 - val_RMSE: 0.4245 - val_loss: 0.0787 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.3016 - RMSE: 0.4173 - loss: 0.0715\n",
      "Epoch 2: val_loss improved from 0.07872 to 0.05032, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.3014 - RMSE: 0.4172 - loss: 0.0714 - val_MAE: 0.2427 - val_RMSE: 0.3474 - val_loss: 0.0503 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2509 - RMSE: 0.3555 - loss: 0.0504\n",
      "Epoch 3: val_loss improved from 0.05032 to 0.04620, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2510 - RMSE: 0.3555 - loss: 0.0504 - val_MAE: 0.2327 - val_RMSE: 0.3302 - val_loss: 0.0462 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.2582 - RMSE: 0.3637 - loss: 0.0520\n",
      "Epoch 4: val_loss did not improve from 0.04620\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 163ms/step - MAE: 0.2582 - RMSE: 0.3638 - loss: 0.0521 - val_MAE: 0.2386 - val_RMSE: 0.3402 - val_loss: 0.0482 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - MAE: 0.2358 - RMSE: 0.3423 - loss: 0.0427\n",
      "Epoch 5: val_loss improved from 0.04620 to 0.04443, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 171ms/step - MAE: 0.2359 - RMSE: 0.3424 - loss: 0.0427 - val_MAE: 0.2250 - val_RMSE: 0.3238 - val_loss: 0.0444 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - MAE: 0.2341 - RMSE: 0.3387 - loss: 0.0443\n",
      "Epoch 6: val_loss improved from 0.04443 to 0.03976, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 171ms/step - MAE: 0.2341 - RMSE: 0.3387 - loss: 0.0443 - val_MAE: 0.2169 - val_RMSE: 0.3132 - val_loss: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2172 - RMSE: 0.3201 - loss: 0.0366\n",
      "Epoch 7: val_loss did not improve from 0.03976\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2173 - RMSE: 0.3201 - loss: 0.0366 - val_MAE: 0.2218 - val_RMSE: 0.3230 - val_loss: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2241 - RMSE: 0.3362 - loss: 0.0375\n",
      "Epoch 8: val_loss did not improve from 0.03976\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2240 - RMSE: 0.3361 - loss: 0.0375 - val_MAE: 0.2145 - val_RMSE: 0.3083 - val_loss: 0.0401 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2157 - RMSE: 0.3223 - loss: 0.0357\n",
      "Epoch 9: val_loss did not improve from 0.03976\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2157 - RMSE: 0.3223 - loss: 0.0357 - val_MAE: 0.2258 - val_RMSE: 0.3274 - val_loss: 0.0426 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - MAE: 0.2491 - RMSE: 0.3586 - loss: 0.0491\n",
      "Epoch 10: val_loss did not improve from 0.03976\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 175ms/step - MAE: 0.2490 - RMSE: 0.3586 - loss: 0.0491 - val_MAE: 0.2150 - val_RMSE: 0.3160 - val_loss: 0.0408 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - MAE: 0.2260 - RMSE: 0.3316 - loss: 0.0405\n",
      "Epoch 11: val_loss improved from 0.03976 to 0.03900, saving model to Transformer_weights_and_norm\\transformer_HSIC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 174ms/step - MAE: 0.2260 - RMSE: 0.3316 - loss: 0.0405 - val_MAE: 0.2141 - val_RMSE: 0.3090 - val_loss: 0.0390 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - MAE: 0.2225 - RMSE: 0.3312 - loss: 0.0383\n",
      "Epoch 12: val_loss did not improve from 0.03900\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 174ms/step - MAE: 0.2225 - RMSE: 0.3312 - loss: 0.0383 - val_MAE: 0.2145 - val_RMSE: 0.3147 - val_loss: 0.0395 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.2028 - RMSE: 0.3146 - loss: 0.0329\n",
      "Epoch 13: val_loss did not improve from 0.03900\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.2028 - RMSE: 0.3146 - loss: 0.0329 - val_MAE: 0.2114 - val_RMSE: 0.3120 - val_loss: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - MAE: 0.2629 - RMSE: 0.3687 - loss: 0.0560\n",
      "Epoch 14: val_loss did not improve from 0.03900\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.2630 - RMSE: 0.3688 - loss: 0.0560 - val_MAE: 0.2414 - val_RMSE: 0.3460 - val_loss: 0.0484 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - MAE: 0.2283 - RMSE: 0.3377 - loss: 0.0393\n",
      "Epoch 15: val_loss did not improve from 0.03900\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 173ms/step - MAE: 0.2282 - RMSE: 0.3377 - loss: 0.0393 - val_MAE: 0.2311 - val_RMSE: 0.3227 - val_loss: 0.0433 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - MAE: 0.2341 - RMSE: 0.3410 - loss: 0.0422\n",
      "Epoch 16: val_loss did not improve from 0.03900\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.2341 - RMSE: 0.3409 - loss: 0.0421 - val_MAE: 0.2264 - val_RMSE: 0.3223 - val_loss: 0.0420 - learning_rate: 0.0010\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "HSIC: Training took 6.73 minutes\n",
      "HSIC: best val_loss=0.038997 @ epoch 11 (val_MAE=0.2141, val_RMSE=0.3090)\n",
      "PCC Shapes: X_train (580, 128, 128, 7), y_train (580, 128, 128, 1), X_val (64, 128, 128, 7), y_val (64, 128, 128, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - MAE: 0.4909 - RMSE: 0.6275 - loss: 0.1853\n",
      "Epoch 1: val_loss improved from inf to 0.09308, saving model to Transformer_weights_and_norm\\transformer_PCC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 168ms/step - MAE: 0.4902 - RMSE: 0.6268 - loss: 0.1849 - val_MAE: 0.3509 - val_RMSE: 0.4593 - val_loss: 0.0931 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - MAE: 0.3103 - RMSE: 0.4137 - loss: 0.0732\n",
      "Epoch 2: val_loss improved from 0.09308 to 0.06642, saving model to Transformer_weights_and_norm\\transformer_PCC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 159ms/step - MAE: 0.3103 - RMSE: 0.4136 - loss: 0.0732 - val_MAE: 0.2849 - val_RMSE: 0.3883 - val_loss: 0.0664 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2839 - RMSE: 0.3839 - loss: 0.0616\n",
      "Epoch 3: val_loss did not improve from 0.06642\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 164ms/step - MAE: 0.2839 - RMSE: 0.3839 - loss: 0.0617 - val_MAE: 0.2888 - val_RMSE: 0.3928 - val_loss: 0.0683 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - MAE: 0.2834 - RMSE: 0.3853 - loss: 0.0613\n",
      "Epoch 4: val_loss did not improve from 0.06642\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 160ms/step - MAE: 0.2834 - RMSE: 0.3854 - loss: 0.0613 - val_MAE: 0.3151 - val_RMSE: 0.4167 - val_loss: 0.0758 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - MAE: 0.2969 - RMSE: 0.3998 - loss: 0.0654\n",
      "Epoch 5: val_loss improved from 0.06642 to 0.05639, saving model to Transformer_weights_and_norm\\transformer_PCC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 163ms/step - MAE: 0.2968 - RMSE: 0.3997 - loss: 0.0654 - val_MAE: 0.2580 - val_RMSE: 0.3604 - val_loss: 0.0564 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2588 - RMSE: 0.3663 - loss: 0.0529\n",
      "Epoch 6: val_loss did not improve from 0.05639\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2588 - RMSE: 0.3662 - loss: 0.0529 - val_MAE: 0.2879 - val_RMSE: 0.3921 - val_loss: 0.0674 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.2599 - RMSE: 0.3578 - loss: 0.0537\n",
      "Epoch 7: val_loss improved from 0.05639 to 0.05243, saving model to Transformer_weights_and_norm\\transformer_PCC.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2600 - RMSE: 0.3578 - loss: 0.0537 - val_MAE: 0.2514 - val_RMSE: 0.3519 - val_loss: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2616 - RMSE: 0.3679 - loss: 0.0542\n",
      "Epoch 8: val_loss did not improve from 0.05243\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 164ms/step - MAE: 0.2615 - RMSE: 0.3679 - loss: 0.0542 - val_MAE: 0.2955 - val_RMSE: 0.3888 - val_loss: 0.0676 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2583 - RMSE: 0.3616 - loss: 0.0513\n",
      "Epoch 9: val_loss did not improve from 0.05243\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2583 - RMSE: 0.3616 - loss: 0.0514 - val_MAE: 0.2629 - val_RMSE: 0.3654 - val_loss: 0.0575 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2536 - RMSE: 0.3589 - loss: 0.0521\n",
      "Epoch 10: val_loss did not improve from 0.05243\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2537 - RMSE: 0.3590 - loss: 0.0521 - val_MAE: 0.3057 - val_RMSE: 0.4316 - val_loss: 0.0765 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - MAE: 0.2983 - RMSE: 0.4041 - loss: 0.0689\n",
      "Epoch 11: val_loss did not improve from 0.05243\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.2982 - RMSE: 0.4041 - loss: 0.0689 - val_MAE: 0.2765 - val_RMSE: 0.3833 - val_loss: 0.0615 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.2629 - RMSE: 0.3648 - loss: 0.0551\n",
      "Epoch 12: val_loss did not improve from 0.05243\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - MAE: 0.2629 - RMSE: 0.3648 - loss: 0.0551 - val_MAE: 0.2650 - val_RMSE: 0.3669 - val_loss: 0.0572 - learning_rate: 0.0010\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "PCC: Training took 4.92 minutes\n",
      "PCC: best val_loss=0.052429 @ epoch 7 (val_MAE=0.2514, val_RMSE=0.3519)\n",
      "MI Shapes: X_train (580, 128, 128, 8), y_train (580, 128, 128, 1), X_val (64, 128, 128, 8), y_val (64, 128, 128, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.5627 - RMSE: 0.7630 - loss: 0.2509\n",
      "Epoch 1: val_loss improved from inf to 0.07091, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 174ms/step - MAE: 0.5617 - RMSE: 0.7617 - loss: 0.2501 - val_MAE: 0.3003 - val_RMSE: 0.4104 - val_loss: 0.0709 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.2929 - RMSE: 0.4079 - loss: 0.0654\n",
      "Epoch 2: val_loss improved from 0.07091 to 0.05736, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.2928 - RMSE: 0.4078 - loss: 0.0653 - val_MAE: 0.2587 - val_RMSE: 0.3711 - val_loss: 0.0574 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.2591 - RMSE: 0.3698 - loss: 0.0537\n",
      "Epoch 3: val_loss did not improve from 0.05736\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 163ms/step - MAE: 0.2591 - RMSE: 0.3699 - loss: 0.0537 - val_MAE: 0.2781 - val_RMSE: 0.3777 - val_loss: 0.0637 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.2766 - RMSE: 0.3883 - loss: 0.0625\n",
      "Epoch 4: val_loss did not improve from 0.05736\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.2765 - RMSE: 0.3882 - loss: 0.0625 - val_MAE: 0.2646 - val_RMSE: 0.3753 - val_loss: 0.0585 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.2670 - RMSE: 0.3742 - loss: 0.0556\n",
      "Epoch 5: val_loss improved from 0.05736 to 0.05239, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2670 - RMSE: 0.3742 - loss: 0.0556 - val_MAE: 0.2474 - val_RMSE: 0.3535 - val_loss: 0.0524 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.2516 - RMSE: 0.3614 - loss: 0.0494\n",
      "Epoch 6: val_loss improved from 0.05239 to 0.04918, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.2516 - RMSE: 0.3614 - loss: 0.0494 - val_MAE: 0.2409 - val_RMSE: 0.3361 - val_loss: 0.0492 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2511 - RMSE: 0.3516 - loss: 0.0506\n",
      "Epoch 7: val_loss did not improve from 0.04918\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2511 - RMSE: 0.3517 - loss: 0.0506 - val_MAE: 0.2441 - val_RMSE: 0.3374 - val_loss: 0.0506 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2586 - RMSE: 0.3657 - loss: 0.0517\n",
      "Epoch 8: val_loss did not improve from 0.04918\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2586 - RMSE: 0.3658 - loss: 0.0517 - val_MAE: 0.2525 - val_RMSE: 0.3564 - val_loss: 0.0567 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2495 - RMSE: 0.3589 - loss: 0.0496\n",
      "Epoch 9: val_loss did not improve from 0.04918\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2494 - RMSE: 0.3588 - loss: 0.0495 - val_MAE: 0.2404 - val_RMSE: 0.3333 - val_loss: 0.0493 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2365 - RMSE: 0.3464 - loss: 0.0447\n",
      "Epoch 10: val_loss improved from 0.04918 to 0.04405, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2365 - RMSE: 0.3464 - loss: 0.0447 - val_MAE: 0.2275 - val_RMSE: 0.3273 - val_loss: 0.0440 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2242 - RMSE: 0.3292 - loss: 0.0408\n",
      "Epoch 11: val_loss improved from 0.04405 to 0.04094, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2242 - RMSE: 0.3292 - loss: 0.0408 - val_MAE: 0.2202 - val_RMSE: 0.3233 - val_loss: 0.0409 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2250 - RMSE: 0.3347 - loss: 0.0388\n",
      "Epoch 12: val_loss improved from 0.04094 to 0.04025, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2251 - RMSE: 0.3347 - loss: 0.0389 - val_MAE: 0.2153 - val_RMSE: 0.3152 - val_loss: 0.0403 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2264 - RMSE: 0.3350 - loss: 0.0406\n",
      "Epoch 13: val_loss improved from 0.04025 to 0.03938, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2264 - RMSE: 0.3350 - loss: 0.0406 - val_MAE: 0.2131 - val_RMSE: 0.3152 - val_loss: 0.0394 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.2330 - RMSE: 0.3435 - loss: 0.0423\n",
      "Epoch 14: val_loss did not improve from 0.03938\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 164ms/step - MAE: 0.2329 - RMSE: 0.3434 - loss: 0.0423 - val_MAE: 0.2298 - val_RMSE: 0.3281 - val_loss: 0.0440 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.2201 - RMSE: 0.3216 - loss: 0.0386\n",
      "Epoch 15: val_loss did not improve from 0.03938\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.2201 - RMSE: 0.3217 - loss: 0.0386 - val_MAE: 0.2206 - val_RMSE: 0.3201 - val_loss: 0.0419 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2267 - RMSE: 0.3386 - loss: 0.0402\n",
      "Epoch 16: val_loss improved from 0.03938 to 0.03855, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.2267 - RMSE: 0.3386 - loss: 0.0402 - val_MAE: 0.2147 - val_RMSE: 0.3181 - val_loss: 0.0385 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.2180 - RMSE: 0.3283 - loss: 0.0367\n",
      "Epoch 17: val_loss improved from 0.03855 to 0.03850, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.2180 - RMSE: 0.3283 - loss: 0.0367 - val_MAE: 0.2134 - val_RMSE: 0.3213 - val_loss: 0.0385 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.2156 - RMSE: 0.3304 - loss: 0.0354\n",
      "Epoch 18: val_loss improved from 0.03850 to 0.03632, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2156 - RMSE: 0.3303 - loss: 0.0354 - val_MAE: 0.2076 - val_RMSE: 0.3159 - val_loss: 0.0363 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2081 - RMSE: 0.3141 - loss: 0.0340\n",
      "Epoch 19: val_loss did not improve from 0.03632\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2081 - RMSE: 0.3142 - loss: 0.0340 - val_MAE: 0.3313 - val_RMSE: 0.4472 - val_loss: 0.0858 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.2582 - RMSE: 0.3709 - loss: 0.0496\n",
      "Epoch 20: val_loss did not improve from 0.03632\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - MAE: 0.2580 - RMSE: 0.3708 - loss: 0.0496 - val_MAE: 0.2211 - val_RMSE: 0.3169 - val_loss: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2105 - RMSE: 0.3051 - loss: 0.0352\n",
      "Epoch 21: val_loss did not improve from 0.03632\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.2106 - RMSE: 0.3052 - loss: 0.0352 - val_MAE: 0.2117 - val_RMSE: 0.3064 - val_loss: 0.0383 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2098 - RMSE: 0.3097 - loss: 0.0349\n",
      "Epoch 22: val_loss did not improve from 0.03632\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - MAE: 0.2098 - RMSE: 0.3098 - loss: 0.0349 - val_MAE: 0.2080 - val_RMSE: 0.3099 - val_loss: 0.0370 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.2005 - RMSE: 0.3073 - loss: 0.0318\n",
      "Epoch 23: val_loss improved from 0.03632 to 0.03343, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - MAE: 0.2005 - RMSE: 0.3073 - loss: 0.0318 - val_MAE: 0.1966 - val_RMSE: 0.2927 - val_loss: 0.0334 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.1936 - RMSE: 0.2966 - loss: 0.0302\n",
      "Epoch 24: val_loss improved from 0.03343 to 0.03337, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1936 - RMSE: 0.2967 - loss: 0.0302 - val_MAE: 0.1997 - val_RMSE: 0.2964 - val_loss: 0.0334 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.1989 - RMSE: 0.2991 - loss: 0.0314\n",
      "Epoch 25: val_loss improved from 0.03337 to 0.03248, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.1989 - RMSE: 0.2992 - loss: 0.0314 - val_MAE: 0.1946 - val_RMSE: 0.2881 - val_loss: 0.0325 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.1967 - RMSE: 0.3037 - loss: 0.0298\n",
      "Epoch 26: val_loss improved from 0.03248 to 0.03145, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.1967 - RMSE: 0.3037 - loss: 0.0298 - val_MAE: 0.1899 - val_RMSE: 0.2820 - val_loss: 0.0315 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - MAE: 0.1885 - RMSE: 0.2951 - loss: 0.0270\n",
      "Epoch 27: val_loss did not improve from 0.03145\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.1886 - RMSE: 0.2951 - loss: 0.0270 - val_MAE: 0.2066 - val_RMSE: 0.3018 - val_loss: 0.0354 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - MAE: 0.2000 - RMSE: 0.3042 - loss: 0.0324\n",
      "Epoch 28: val_loss improved from 0.03145 to 0.02850, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 173ms/step - MAE: 0.2000 - RMSE: 0.3042 - loss: 0.0324 - val_MAE: 0.1841 - val_RMSE: 0.2822 - val_loss: 0.0285 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - MAE: 0.1806 - RMSE: 0.2868 - loss: 0.0258\n",
      "Epoch 29: val_loss did not improve from 0.02850\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 172ms/step - MAE: 0.1807 - RMSE: 0.2869 - loss: 0.0258 - val_MAE: 0.1883 - val_RMSE: 0.2880 - val_loss: 0.0296 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - MAE: 0.1794 - RMSE: 0.2861 - loss: 0.0248\n",
      "Epoch 30: val_loss improved from 0.02850 to 0.02810, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1794 - RMSE: 0.2861 - loss: 0.0248 - val_MAE: 0.1825 - val_RMSE: 0.2789 - val_loss: 0.0281 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.1779 - RMSE: 0.2976 - loss: 0.0226\n",
      "Epoch 31: val_loss improved from 0.02810 to 0.02543, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.1779 - RMSE: 0.2975 - loss: 0.0226 - val_MAE: 0.1755 - val_RMSE: 0.2753 - val_loss: 0.0254 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.1762 - RMSE: 0.2871 - loss: 0.0231\n",
      "Epoch 32: val_loss did not improve from 0.02543\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1763 - RMSE: 0.2871 - loss: 0.0231 - val_MAE: 0.1766 - val_RMSE: 0.2707 - val_loss: 0.0262 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - MAE: 0.1912 - RMSE: 0.2958 - loss: 0.0277\n",
      "Epoch 33: val_loss did not improve from 0.02543\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1912 - RMSE: 0.2958 - loss: 0.0277 - val_MAE: 0.1859 - val_RMSE: 0.2873 - val_loss: 0.0286 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.1854 - RMSE: 0.2959 - loss: 0.0258\n",
      "Epoch 34: val_loss did not improve from 0.02543\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 165ms/step - MAE: 0.1854 - RMSE: 0.2959 - loss: 0.0258 - val_MAE: 0.1825 - val_RMSE: 0.2758 - val_loss: 0.0275 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.1747 - RMSE: 0.2865 - loss: 0.0215\n",
      "Epoch 35: val_loss improved from 0.02543 to 0.02438, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 170ms/step - MAE: 0.1747 - RMSE: 0.2865 - loss: 0.0215 - val_MAE: 0.1695 - val_RMSE: 0.2602 - val_loss: 0.0244 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.1675 - RMSE: 0.2778 - loss: 0.0206\n",
      "Epoch 36: val_loss did not improve from 0.02438\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.1675 - RMSE: 0.2778 - loss: 0.0206 - val_MAE: 0.1727 - val_RMSE: 0.2583 - val_loss: 0.0251 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - MAE: 0.1622 - RMSE: 0.2641 - loss: 0.0196\n",
      "Epoch 37: val_loss improved from 0.02438 to 0.02361, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1622 - RMSE: 0.2642 - loss: 0.0196 - val_MAE: 0.1694 - val_RMSE: 0.2642 - val_loss: 0.0236 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - MAE: 0.1624 - RMSE: 0.2783 - loss: 0.0185\n",
      "Epoch 38: val_loss improved from 0.02361 to 0.02231, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 171ms/step - MAE: 0.1624 - RMSE: 0.2783 - loss: 0.0185 - val_MAE: 0.1598 - val_RMSE: 0.2403 - val_loss: 0.0223 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.1527 - RMSE: 0.2473 - loss: 0.0179\n",
      "Epoch 39: val_loss improved from 0.02231 to 0.02093, saving model to Transformer_weights_and_norm\\transformer_MI.tmp.weights.h5\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.1528 - RMSE: 0.2474 - loss: 0.0179 - val_MAE: 0.1579 - val_RMSE: 0.2348 - val_loss: 0.0209 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - MAE: 0.1508 - RMSE: 0.2494 - loss: 0.0168\n",
      "Epoch 40: val_loss did not improve from 0.02093\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 166ms/step - MAE: 0.1508 - RMSE: 0.2495 - loss: 0.0168 - val_MAE: 0.1627 - val_RMSE: 0.2473 - val_loss: 0.0221 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - MAE: 0.1591 - RMSE: 0.2736 - loss: 0.0175\n",
      "Epoch 41: val_loss did not improve from 0.02093\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 168ms/step - MAE: 0.1591 - RMSE: 0.2735 - loss: 0.0175 - val_MAE: 0.1609 - val_RMSE: 0.2496 - val_loss: 0.0217 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - MAE: 0.1670 - RMSE: 0.2877 - loss: 0.0185\n",
      "Epoch 42: val_loss did not improve from 0.02093\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 176ms/step - MAE: 0.1670 - RMSE: 0.2875 - loss: 0.0185 - val_MAE: 0.1658 - val_RMSE: 0.2664 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - MAE: 0.2196 - RMSE: 0.3395 - loss: 0.0394\n",
      "Epoch 43: val_loss did not improve from 0.02093\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 179ms/step - MAE: 0.2200 - RMSE: 0.3398 - loss: 0.0395 - val_MAE: 0.3605 - val_RMSE: 0.4989 - val_loss: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - MAE: 0.3264 - RMSE: 0.4416 - loss: 0.0805\n",
      "Epoch 44: val_loss did not improve from 0.02093\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 190ms/step - MAE: 0.3262 - RMSE: 0.4415 - loss: 0.0805 - val_MAE: 0.2983 - val_RMSE: 0.4198 - val_loss: 0.0718 - learning_rate: 0.0010\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "MI: Training took 18.08 minutes\n",
      "MI: best val_loss=0.020930 @ epoch 39 (val_MAE=0.1579, val_RMSE=0.2348)\n"
     ]
    }
   ],
   "source": [
    "# Train models with each feature set. Do this in loop so the code is not so long\n",
    "\n",
    "# Config\n",
    "FEATURESETS = [\n",
    "    (\"HSIC\", h5_path_HSIC),\n",
    "    (\"PCC\",  h5_path_PCC),\n",
    "    (\"MI\",   h5_path_MI),\n",
    "]\n",
    "PATCH_SIZE   = 128\n",
    "STRIDE       = 64\n",
    "MIN_VALID_FR = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "LR           = 1e-3\n",
    "EPOCHS       = 50\n",
    "BATCH_SIZE   = 4\n",
    "EPS          = 1e-6\n",
    "\n",
    "# Where to store temporary artifacts\n",
    "ARTIFACTS_DIR = \"Transformer_weights_and_norm\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Final (winner) artifacts\n",
    "FINAL_NORM    = os.path.join(ARTIFACTS_DIR, \"best_norm_stats.npz\")\n",
    "FINAL_WEIGHTS = os.path.join(ARTIFACTS_DIR, \"transformer_best.weights.h5\")\n",
    "FINAL_TAG     = os.path.join(ARTIFACTS_DIR, \"transformer_best.tag.txt\")\n",
    "\n",
    "# Train models with each feature set (Transformer) — minimize Huber loss, report MAE/RMSE\n",
    "results = []  # (name, best_val_loss, train_time_min, tmp_norm_path, tmp_weights_path)\n",
    "\n",
    "for name, h5_path in FEATURESETS:\n",
    "    # Data split\n",
    "    (X_train, y_train, m_train), (X_val, y_val, m_val), _ = DS.DL_split(\n",
    "        h5_path=str(h5_path),\n",
    "        holdout_aoi=holdout_aoi,\n",
    "        val_fraction=VAL_FRACTION,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=STRIDE,\n",
    "        min_valid_frac=MIN_VALID_FR,\n",
    "    )\n",
    "    print(f\"{name} Shapes: X_train {X_train.shape}, y_train {y_train.shape}, X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "\n",
    "    # Train-only stats (NaN-safe) + normalize\n",
    "    tmp_norm_path = os.path.join(ARTIFACTS_DIR, f\"{name}_norm_stats.tmp.npz\")\n",
    "    mean = np.nanmean(X_train, axis=(0, 1, 2), keepdims=True).astype(\"float32\")\n",
    "    std  = (np.nanstd (X_train, axis=(0, 1, 2), keepdims=True) + EPS).astype(\"float32\")\n",
    "    std  = np.where(std < EPS, 1.0, std).astype(\"float32\")\n",
    "    np.savez(tmp_norm_path, mean=mean, std=std)\n",
    "\n",
    "    with np.load(tmp_norm_path) as f:\n",
    "        mu, sigma = f[\"mean\"], f[\"std\"]\n",
    "    X_train = np.nan_to_num(X_train, nan=mu, posinf=mu, neginf=mu).astype(\"float32\")\n",
    "    X_val   = np.nan_to_num(X_val,   nan=mu, posinf=mu, neginf=mu).astype(\"float32\")\n",
    "    X_train_n = ((X_train - mu) / sigma).astype(\"float32\")\n",
    "    X_val_n   = ((X_val   - mu) / sigma).astype(\"float32\")\n",
    "\n",
    "    # Labels + masks\n",
    "    y_train_f, w_train = ARCH.fill_nan_and_mask(y_train)\n",
    "    y_val_f,   w_val   = ARCH.fill_nan_and_mask(y_val)\n",
    "    w_train_4d = w_train[..., None].astype(\"float32\")\n",
    "    w_val_4d   = w_val[..., None].astype(\"float32\")\n",
    "\n",
    "    # Model\n",
    "    model = ARCH.transformer_seg_model(\n",
    "        input_shape=X_train_n.shape[1:],  # (H,W,C)\n",
    "        patch_size=16,\n",
    "        d_model=256,\n",
    "        depth=4,\n",
    "        num_heads=4,\n",
    "        mlp_dim=512,\n",
    "        dropout=0.0,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "        loss=tf.keras.losses.Huber(delta=1.0),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(name=\"MAE\"),\n",
    "            tf.keras.metrics.RootMeanSquaredError(name=\"RMSE\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Callbacks: monitor val_loss (Huber)\n",
    "    tmp_weights = os.path.join(ARTIFACTS_DIR, f\"transformer_{name}.tmp.weights.h5\")\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            tmp_weights, monitor=\"val_loss\", mode=\"min\",\n",
    "            save_best_only=True, save_weights_only=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", mode=\"min\",\n",
    "            patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", mode=\"min\",\n",
    "            factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # --- train ---\n",
    "    start_time = time.time()\n",
    "    hist = model.fit(\n",
    "        X_train_n, y_train_f,\n",
    "        sample_weight=w_train_4d,\n",
    "        validation_data=(X_val_n, y_val_f, w_val_4d),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "    train_time = (time.time() - start_time) / 60.0\n",
    "\n",
    "    # --- pick epoch by best val_loss, but also report MAE/RMSE at that epoch ---\n",
    "    best_idx = int(np.argmin(hist.history[\"val_loss\"]))\n",
    "    best_val_loss  = float(hist.history[\"val_loss\"][best_idx])\n",
    "    best_val_mae   = float(hist.history[\"val_MAE\"][best_idx])\n",
    "    best_val_rmse  = float(hist.history[\"val_RMSE\"][best_idx])\n",
    "\n",
    "    print(f\"{name}: Training took {train_time:.2f} minutes\")\n",
    "    print(f\"{name}: best val_loss={best_val_loss:.6f} @ epoch {best_idx+1} \"\n",
    "          f\"(val_MAE={best_val_mae:.4f}, val_RMSE={best_val_rmse:.4f})\")\n",
    "\n",
    "    results.append((name, best_val_loss, train_time, tmp_norm_path, tmp_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a07da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation leaderboard (lower is better)\n",
      "1.       MI  val_loss=0.020930  time=18.08m\n",
      "2.     HSIC  val_loss=0.038997  time=6.73m\n",
      "3.      PCC  val_loss=0.052429  time=4.92m\n",
      "\n",
      "WINNER: MI (val_loss=0.020930)\n",
      "\n",
      "Saved winner norm stats -> Transformer_weights_and_norm\\best_norm_stats.npz\n",
      "Saved winner weights   -> Transformer_weights_and_norm\\transformer_best.weights.h5\n",
      "Wrote tag file         -> Transformer_weights_and_norm\\transformer_best.tag.txt\n"
     ]
    }
   ],
   "source": [
    "# Pick the global winner by validation loss (Huber) — lower is better\n",
    "results_sorted = sorted(results, key=lambda t: t[1])\n",
    "winner_name, winner_loss, winner_time, winner_norm, winner_weights = results_sorted[0]\n",
    "\n",
    "print(\"\\nValidation leaderboard (lower is better)\")\n",
    "for rank, (n, v_loss, tmin, _, _) in enumerate(results_sorted, 1):\n",
    "    print(f\"{rank}. {n:>8s}  val_loss={v_loss:.6f}  time={tmin:.2f}m\")\n",
    "\n",
    "print(f\"\\nWINNER: {winner_name} (val_loss={winner_loss:.6f})\")\n",
    "\n",
    "# Clean previous finals if exist\n",
    "for p in [FINAL_NORM, FINAL_WEIGHTS, FINAL_TAG]:\n",
    "    if os.path.exists(p):\n",
    "        os.remove(p)\n",
    "\n",
    "# Move winning artifacts to final names\n",
    "shutil.move(winner_norm, FINAL_NORM)\n",
    "shutil.move(winner_weights, FINAL_WEIGHTS)\n",
    "with open(FINAL_TAG, \"w\") as f:\n",
    "    f.write(f\"winner={winner_name}\\nval_loss={winner_loss:.6f}\\n\")\n",
    "\n",
    "# Delete other temp files\n",
    "for n, _, _, norm_path, weight_path in results_sorted[1:]:\n",
    "    if os.path.exists(norm_path):\n",
    "        os.remove(norm_path)\n",
    "    if os.path.exists(weight_path):\n",
    "        os.remove(weight_path)\n",
    "\n",
    "print(f\"\\nSaved winner norm stats -> {FINAL_NORM}\")\n",
    "print(f\"Saved winner weights   -> {FINAL_WEIGHTS}\")\n",
    "print(f\"Wrote tag file         -> {FINAL_TAG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7495b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL-features H5 already exists: c:\\Users\\mathi\\Documents\\Paper2\\ML_SD\\data\\h5_dir\\ALL\\data_ALL.h5\n",
      "ALL_TX_REPORT Shapes: X_train (580, 128, 128, 27), y_train (580, 128, 128, 1), X_val (64, 128, 128, 27), y_val (64, 128, 128, 1)\n",
      "Epoch 1/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 230ms/step - MAE: 0.5706 - RMSE: 0.7967 - loss: 0.2559 - val_MAE: 0.3357 - val_RMSE: 0.4523 - val_loss: 0.0899 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 236ms/step - MAE: 0.2933 - RMSE: 0.4093 - loss: 0.0682 - val_MAE: 0.2593 - val_RMSE: 0.3688 - val_loss: 0.0560 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 232ms/step - MAE: 0.2551 - RMSE: 0.3794 - loss: 0.0529 - val_MAE: 0.2765 - val_RMSE: 0.4007 - val_loss: 0.0657 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 223ms/step - MAE: 0.2661 - RMSE: 0.3800 - loss: 0.0565 - val_MAE: 0.2376 - val_RMSE: 0.3537 - val_loss: 0.0515 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 228ms/step - MAE: 0.2353 - RMSE: 0.3511 - loss: 0.0461 - val_MAE: 0.2436 - val_RMSE: 0.3498 - val_loss: 0.0502 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 232ms/step - MAE: 0.2205 - RMSE: 0.3341 - loss: 0.0404 - val_MAE: 0.2164 - val_RMSE: 0.3211 - val_loss: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 221ms/step - MAE: 0.2120 - RMSE: 0.3232 - loss: 0.0366 - val_MAE: 0.2116 - val_RMSE: 0.3310 - val_loss: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 222ms/step - MAE: 0.2109 - RMSE: 0.3242 - loss: 0.0365 - val_MAE: 0.2180 - val_RMSE: 0.3247 - val_loss: 0.0428 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 209ms/step - MAE: 0.2199 - RMSE: 0.3368 - loss: 0.0390 - val_MAE: 0.2490 - val_RMSE: 0.3522 - val_loss: 0.0514 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 206ms/step - MAE: 0.2299 - RMSE: 0.3409 - loss: 0.0405 - val_MAE: 0.2064 - val_RMSE: 0.3175 - val_loss: 0.0390 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 210ms/step - MAE: 0.2152 - RMSE: 0.3310 - loss: 0.0368 - val_MAE: 0.2119 - val_RMSE: 0.3249 - val_loss: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 208ms/step - MAE: 0.2209 - RMSE: 0.3400 - loss: 0.0386 - val_MAE: 0.1975 - val_RMSE: 0.3040 - val_loss: 0.0357 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 205ms/step - MAE: 0.1985 - RMSE: 0.3158 - loss: 0.0328 - val_MAE: 0.1879 - val_RMSE: 0.2991 - val_loss: 0.0321 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 208ms/step - MAE: 0.1807 - RMSE: 0.3008 - loss: 0.0263 - val_MAE: 0.1865 - val_RMSE: 0.2962 - val_loss: 0.0311 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 208ms/step - MAE: 0.1801 - RMSE: 0.2910 - loss: 0.0272 - val_MAE: 0.2029 - val_RMSE: 0.3061 - val_loss: 0.0349 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 208ms/step - MAE: 0.1876 - RMSE: 0.3046 - loss: 0.0284 - val_MAE: 0.2028 - val_RMSE: 0.3035 - val_loss: 0.0357 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 207ms/step - MAE: 0.1867 - RMSE: 0.2999 - loss: 0.0281 - val_MAE: 0.1738 - val_RMSE: 0.2764 - val_loss: 0.0269 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 205ms/step - MAE: 0.1770 - RMSE: 0.2859 - loss: 0.0252 - val_MAE: 0.1816 - val_RMSE: 0.2830 - val_loss: 0.0299 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 206ms/step - MAE: 0.1703 - RMSE: 0.2796 - loss: 0.0229 - val_MAE: 0.1664 - val_RMSE: 0.2639 - val_loss: 0.0253 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 213ms/step - MAE: 0.1661 - RMSE: 0.2749 - loss: 0.0220 - val_MAE: 0.1905 - val_RMSE: 0.3010 - val_loss: 0.0323 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1991 - RMSE: 0.3081 - loss: 0.0321 - val_MAE: 0.2705 - val_RMSE: 0.3902 - val_loss: 0.0622 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 225ms/step - MAE: 0.2817 - RMSE: 0.3995 - loss: 0.0644 - val_MAE: 0.2554 - val_RMSE: 0.3713 - val_loss: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 221ms/step - MAE: 0.2259 - RMSE: 0.3420 - loss: 0.0417 - val_MAE: 0.2057 - val_RMSE: 0.3135 - val_loss: 0.0378 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - MAE: 0.2087 - RMSE: 0.3253 - loss: 0.0348\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 224ms/step - MAE: 0.2087 - RMSE: 0.3252 - loss: 0.0348 - val_MAE: 0.2060 - val_RMSE: 0.3168 - val_loss: 0.0401 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 219ms/step - MAE: 0.1856 - RMSE: 0.3049 - loss: 0.0275 - val_MAE: 0.1768 - val_RMSE: 0.2800 - val_loss: 0.0292 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 219ms/step - MAE: 0.1681 - RMSE: 0.2860 - loss: 0.0220 - val_MAE: 0.1721 - val_RMSE: 0.2779 - val_loss: 0.0275 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1610 - RMSE: 0.2683 - loss: 0.0208 - val_MAE: 0.1658 - val_RMSE: 0.2674 - val_loss: 0.0255 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 217ms/step - MAE: 0.1571 - RMSE: 0.2684 - loss: 0.0208 - val_MAE: 0.1678 - val_RMSE: 0.2671 - val_loss: 0.0259 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1679 - RMSE: 0.2884 - loss: 0.0225 - val_MAE: 0.1642 - val_RMSE: 0.2648 - val_loss: 0.0251 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1505 - RMSE: 0.2553 - loss: 0.0187 - val_MAE: 0.1594 - val_RMSE: 0.2604 - val_loss: 0.0237 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 214ms/step - MAE: 0.1514 - RMSE: 0.2608 - loss: 0.0188 - val_MAE: 0.1571 - val_RMSE: 0.2566 - val_loss: 0.0227 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 214ms/step - MAE: 0.1455 - RMSE: 0.2501 - loss: 0.0167 - val_MAE: 0.1537 - val_RMSE: 0.2513 - val_loss: 0.0215 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 214ms/step - MAE: 0.1431 - RMSE: 0.2499 - loss: 0.0159 - val_MAE: 0.1573 - val_RMSE: 0.2512 - val_loss: 0.0220 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1470 - RMSE: 0.2512 - loss: 0.0178 - val_MAE: 0.1521 - val_RMSE: 0.2506 - val_loss: 0.0210 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 214ms/step - MAE: 0.1361 - RMSE: 0.2369 - loss: 0.0139 - val_MAE: 0.1544 - val_RMSE: 0.2536 - val_loss: 0.0219 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 225ms/step - MAE: 0.1396 - RMSE: 0.2418 - loss: 0.0147 - val_MAE: 0.1505 - val_RMSE: 0.2441 - val_loss: 0.0199 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1468 - RMSE: 0.2583 - loss: 0.0153 - val_MAE: 0.1551 - val_RMSE: 0.2472 - val_loss: 0.0203 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 225ms/step - MAE: 0.1374 - RMSE: 0.2303 - loss: 0.0139 - val_MAE: 0.1602 - val_RMSE: 0.2709 - val_loss: 0.0246 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1467 - RMSE: 0.2531 - loss: 0.0157 - val_MAE: 0.1431 - val_RMSE: 0.2338 - val_loss: 0.0177 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1304 - RMSE: 0.2269 - loss: 0.0120 - val_MAE: 0.1402 - val_RMSE: 0.2289 - val_loss: 0.0166 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 219ms/step - MAE: 0.1279 - RMSE: 0.2191 - loss: 0.0119 - val_MAE: 0.1403 - val_RMSE: 0.2261 - val_loss: 0.0163 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1279 - RMSE: 0.2238 - loss: 0.0115 - val_MAE: 0.1432 - val_RMSE: 0.2332 - val_loss: 0.0171 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 223ms/step - MAE: 0.1342 - RMSE: 0.2282 - loss: 0.0125 - val_MAE: 0.1429 - val_RMSE: 0.2322 - val_loss: 0.0173 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1316 - RMSE: 0.2202 - loss: 0.0125 - val_MAE: 0.1419 - val_RMSE: 0.2315 - val_loss: 0.0164 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 219ms/step - MAE: 0.1345 - RMSE: 0.2399 - loss: 0.0120 - val_MAE: 0.1415 - val_RMSE: 0.2288 - val_loss: 0.0166 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 220ms/step - MAE: 0.1239 - RMSE: 0.2207 - loss: 0.0099 - val_MAE: 0.1286 - val_RMSE: 0.2176 - val_loss: 0.0136 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 222ms/step - MAE: 0.1163 - RMSE: 0.2144 - loss: 0.0086 - val_MAE: 0.1317 - val_RMSE: 0.2265 - val_loss: 0.0148 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 221ms/step - MAE: 0.1224 - RMSE: 0.2287 - loss: 0.0093 - val_MAE: 0.1274 - val_RMSE: 0.2134 - val_loss: 0.0132 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 218ms/step - MAE: 0.1142 - RMSE: 0.2077 - loss: 0.0084 - val_MAE: 0.1352 - val_RMSE: 0.2243 - val_loss: 0.0151 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 216ms/step - MAE: 0.1216 - RMSE: 0.2156 - loss: 0.0100 - val_MAE: 0.1625 - val_RMSE: 0.2468 - val_loss: 0.0218 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 48.\n",
      "\n",
      "[ALL_TX_REPORT] best val_loss=0.013158 @ epoch 48 (val_MAE=0.1274, val_RMSE=0.2134); train_time=26.51 min\n"
     ]
    }
   ],
   "source": [
    "# Transformer with ALL features (Ablation study)\n",
    "name = \"ALL_TX_REPORT\"\n",
    "\n",
    "# Ensure ALL-features H5 exists\n",
    "h5_path_ALL = h5_dir / \"ALL\" / \"data_ALL.h5\"\n",
    "if not h5_path_ALL.exists():\n",
    "    (h5_dir / \"ALL\").mkdir(parents=True, exist_ok=True)\n",
    "    DL.build_h5(\n",
    "        data_dir=str(ROOT / \"data\" / \"tif_files\"),\n",
    "        out_dir=str(h5_dir / \"ALL\"),\n",
    "        write_mask=True,\n",
    "        selected_features=FEATURE_NAMES,  # full list\n",
    "        out_name=\"data_ALL.h5\",\n",
    "    )\n",
    "    print(f\"Wrote ALL-features H5: {h5_path_ALL}\")\n",
    "else:\n",
    "    print(f\"ALL-features H5 already exists: {h5_path_ALL}\")\n",
    "\n",
    "# Split (dev train/val + holdout AOI)\n",
    "(X_train, y_train, m_train), (X_val, y_val, m_val), _ = DS.DL_split(\n",
    "    h5_path=str(h5_path_ALL),\n",
    "    holdout_aoi=holdout_aoi,\n",
    "    val_fraction=VAL_FRACTION,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    stride=STRIDE,\n",
    "    min_valid_frac=MIN_VALID_FR,\n",
    ")\n",
    "print(f\"{name} Shapes: X_train {X_train.shape}, y_train {y_train.shape}, X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "\n",
    "# NaN-safe normalization from TRAIN only\n",
    "mu  = np.nanmean(X_train, axis=(0, 1, 2), keepdims=True).astype(\"float32\")\n",
    "std = (np.nanstd (X_train, axis=(0, 1, 2), keepdims=True) + EPS).astype(\"float32\")\n",
    "std = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "\n",
    "X_train = np.nan_to_num(X_train, nan=mu, posinf=mu, neginf=mu).astype(\"float32\")\n",
    "X_val   = np.nan_to_num(X_val,   nan=mu, posinf=mu, neginf=mu).astype(\"float32\")\n",
    "X_train_n = ((X_train - mu) / std).astype(\"float32\")\n",
    "X_val_n   = ((X_val   - mu) / std).astype(\"float32\")\n",
    "\n",
    "# Labels + masks\n",
    "y_train_f, w_train = ARCH.fill_nan_and_mask(y_train)\n",
    "y_val_f,   w_val   = ARCH.fill_nan_and_mask(y_val)\n",
    "w_train_4d = w_train[..., None].astype(\"float32\")\n",
    "w_val_4d   = w_val[..., None].astype(\"float32\")\n",
    "\n",
    "# Build and compile Transformer\n",
    "model = ARCH.transformer_seg_model(\n",
    "    input_shape=X_train_n.shape[1:], \n",
    "    patch_size=16,\n",
    "    d_model=256,\n",
    "    depth=4,\n",
    "    num_heads=4,\n",
    "    mlp_dim=512,\n",
    "    dropout=0.0,\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=tf.keras.losses.Huber(delta=1.0),                 \n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"MAE\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(name=\"RMSE\")],\n",
    ")\n",
    "\n",
    "# Train and monitor val_loss only\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                                     patience=10, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
    "                                         factor=0.5, patience=5, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "hist = model.fit(\n",
    "    X_train_n, y_train_f,\n",
    "    sample_weight=w_train_4d,\n",
    "    validation_data=(X_val_n, y_val_f, w_val_4d),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "train_time = (time.time() - start_time) / 60.0\n",
    "\n",
    "# Report best-loss epoch with MAE/RMSE\n",
    "best_idx      = int(np.argmin(hist.history[\"val_loss\"]))\n",
    "best_val_loss = float(hist.history[\"val_loss\"][best_idx])\n",
    "best_val_mae  = float(hist.history[\"val_MAE\"][best_idx])\n",
    "best_val_rmse = float(hist.history[\"val_RMSE\"][best_idx])\n",
    "\n",
    "print(f\"\\n[{name}] best val_loss={best_val_loss:.6f} @ epoch {best_idx+1} \"\n",
    "      f\"(val_MAE={best_val_mae:.4f}, val_RMSE={best_val_rmse:.4f}); \"\n",
    "      f\"train_time={train_time:.2f} min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
